{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f8fa09",
   "metadata": {},
   "source": [
    "# Image Captioning with CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c620be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1b50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the pre-trained CNN model (InceptionV3)\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.keras.utils.load_img(image_path, target_size=(299, 299))  # Resize image\n",
    "    image = tf.keras.utils.img_to_array(image)  # Convert to array\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    image = tf.keras.applications.inception_v3.preprocess_input(image)  # Preprocess for InceptionV3\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20d98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load InceptionV3 and extract features\n",
    "inception_model = InceptionV3(weights=\"imagenet\")\n",
    "cnn_model = Model(inception_model.input, inception_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af078da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract image features\n",
    "def extract_features(image_path, model):\n",
    "    image = preprocess_image(image_path)\n",
    "    features = model.predict(image)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a85b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM-based language model\n",
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(2048,))  # CNN features\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation=\"relu\")(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_length,))  # Text input\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    decoder1 = Add()([fe2, se3])\n",
    "    decoder2 = Dense(256, activation=\"relu\")(decoder1)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(decoder2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02415958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Dataset for Text Captions\n",
    "captions = {\n",
    "    \"image1.jpg\": [\"a dog running in a park\", \"a canine playing outdoors\"],\n",
    "    \"image2.jpg\": [\"a cat sitting on a couch\", \"a feline relaxing indoors\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1cdca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions\n",
    "tokenizer = Tokenizer()\n",
    "all_captions = [caption for captions_list in captions.values() for caption in captions_list]\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9629e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "\n",
    "def create_sequences(tokenizer, max_length, descriptions, photo_features):\n",
    "    X1, X2, y = [], [], []\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                X1.append(photo_features[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ac3495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n"
     ]
    }
   ],
   "source": [
    "# Example image features\n",
    "photo_features = {\n",
    "    \"image1.jpg\": extract_features(\"C:/Users/swaro/30 day challenge/Day21/image1.jpg\", cnn_model),\n",
    "    \"image2.jpg\": extract_features(\"C:/Users/swaro/30 day challenge/Day21/image2.jpg\", cnn_model),\n",
    "}\n",
    "\n",
    "X1, X2, y = create_sequences(tokenizer, max_length, captions, photo_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45c9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06455e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 3.6764\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8143\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.7490\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6901\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.2940\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.3747\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.4419\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.3096\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.1294\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.1165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a75c34aa90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model (dummy training for demonstration)\n",
    "model.fit([X1, X2], y, epochs=10, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "260f5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption Generation\n",
    "def generate_caption(model, tokenizer, photo_feature, max_length):\n",
    "    input_text = \"startseq\"\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo_feature, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word[yhat]\n",
    "        if word is None or word == \"endseq\":\n",
    "            break\n",
    "        input_text += \" \" + word\n",
    "    return input_text.split(\" \")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "832b5c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "Generated Caption: cat cat couch couch couch couch\n"
     ]
    }
   ],
   "source": [
    "# Test with an example\n",
    "test_image_feature = extract_features(\"C:/Users/swaro/30 day challenge/Day21/test image.jpg\", cnn_model)\n",
    "caption = generate_caption(model, tokenizer, test_image_feature, max_length)\n",
    "print(\"Generated Caption:\", \" \".join(caption))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
